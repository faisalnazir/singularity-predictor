import{c as yt,a as V,b as pe,u as bt,d as Y,e as w,S as ee,f as Me,o as vt,g as wt,F as W,h as At}from"./vendor-g-PKEbsL.js";import{s as ue,l as xe,a as Qe,c as et,b as It,m as xt}from"./d3-4E-2E64W.js";(function(){const e=document.createElement("link").relList;if(e&&e.supports&&e.supports("modulepreload"))return;for(const i of document.querySelectorAll('link[rel="modulepreload"]'))o(i);new MutationObserver(i=>{for(const r of i)if(r.type==="childList")for(const s of r.addedNodes)s.tagName==="LINK"&&s.rel==="modulepreload"&&o(s)}).observe(document,{childList:!0,subtree:!0});function a(i){const r={};return i.integrity&&(r.integrity=i.integrity),i.referrerPolicy&&(r.referrerPolicy=i.referrerPolicy),i.crossOrigin==="use-credentials"?r.credentials="include":i.crossOrigin==="anonymous"?r.credentials="omit":r.credentials="same-origin",r}function o(i){if(i.ep)return;i.ep=!0;const r=a(i);fetch(i.href,r)}})();const St=t=>pe(()=>t());function kt(t,e,a){let o=a.length,i=e.length,r=o,s=0,n=0,c=e[i-1].nextSibling,m=null;for(;s<i||n<r;){if(e[s]===a[n]){s++,n++;continue}for(;e[i-1]===a[r-1];)i--,r--;if(i===s){const g=r<o?n?a[n-1].nextSibling:a[r-n]:c;for(;n<r;)t.insertBefore(a[n++],g)}else if(r===n)for(;s<i;)(!m||!m.has(e[s]))&&e[s].remove(),s++;else if(e[s]===a[r-1]&&a[n]===e[i-1]){const g=e[--i].nextSibling;t.insertBefore(a[n++],e[s++].nextSibling),t.insertBefore(a[--r],g),e[i]=a[r]}else{if(!m){m=new Map;let x=n;for(;x<r;)m.set(a[x],x++)}const g=m.get(e[s]);if(g!=null)if(n<g&&g<r){let x=s,C=1,H;for(;++x<i&&x<r&&!((H=m.get(e[x]))==null||H!==g+C);)C++;if(C>g-n){const te=e[s];for(;n<g;)t.insertBefore(a[n++],te)}else t.replaceChild(a[n++],e[s++])}else s++;else e[s++].remove()}}}const tt="_$DX_DELEGATE";function $t(t,e,a,o={}){let i;return yt(r=>{i=r,e===document?t():h(e,t(),e.firstChild?null:void 0,a)},o.owner),()=>{i(),e.textContent=""}}function A(t,e,a,o){let i;const r=()=>{const n=document.createElement("template");return n.innerHTML=t,n.content.firstChild},s=()=>(i||(i=r())).cloneNode(!0);return s.cloneNode=s,s}function Ye(t,e=window.document){const a=e[tt]||(e[tt]=new Set);for(let o=0,i=t.length;o<i;o++){const r=t[o];a.has(r)||(a.add(r),e.addEventListener(r,_t))}}function at(t,e,a){a==null?t.removeAttribute(e):t.setAttribute(e,a)}function Le(t,e){e==null?t.removeAttribute("class"):t.className=e}function Pe(t,e,a){a!=null?t.style.setProperty(e,a):t.style.removeProperty(e)}function Tt(t,e,a){return bt(()=>t(e,a))}function h(t,e,a,o){if(a!==void 0&&!o&&(o=[]),typeof e!="function")return $e(t,e,o,a);V(i=>$e(t,e(),i,a),o)}function _t(t){let e=t.target;const a=`$$${t.type}`,o=t.target,i=t.currentTarget,r=c=>Object.defineProperty(t,"target",{configurable:!0,value:c}),s=()=>{const c=e[a];if(c&&!e.disabled){const m=e[`${a}Data`];if(m!==void 0?c.call(e,m,t):c.call(e,t),t.cancelBubble)return}return e.host&&typeof e.host!="string"&&!e.host._$host&&e.contains(t.target)&&r(e.host),!0},n=()=>{for(;s()&&(e=e._$host||e.parentNode||e.host););};if(Object.defineProperty(t,"currentTarget",{configurable:!0,get(){return e||document}}),t.composedPath){const c=t.composedPath();r(c[0]);for(let m=0;m<c.length-2&&(e=c[m],!!s());m++){if(e._$host){e=e._$host,n();break}if(e.parentNode===i)break}}else n();r(o)}function $e(t,e,a,o,i){for(;typeof a=="function";)a=a();if(e===a)return a;const r=typeof e,s=o!==void 0;if(t=s&&a[0]&&a[0].parentNode||t,r==="string"||r==="number"){if(r==="number"&&(e=e.toString(),e===a))return a;if(s){let n=a[0];n&&n.nodeType===3?n.data!==e&&(n.data=e):n=document.createTextNode(e),a=re(t,a,o,n)}else a!==""&&typeof a=="string"?a=t.firstChild.data=e:a=t.textContent=e}else if(e==null||r==="boolean")a=re(t,a,o);else{if(r==="function")return V(()=>{let n=e();for(;typeof n=="function";)n=n();a=$e(t,n,a,o)}),()=>a;if(Array.isArray(e)){const n=[],c=a&&Array.isArray(a);if(Ne(n,e,a,i))return V(()=>a=$e(t,n,a,o,!0)),()=>a;if(n.length===0){if(a=re(t,a,o),s)return a}else c?a.length===0?it(t,n,o):kt(t,a,n):(a&&re(t),it(t,n));a=n}else if(e.nodeType){if(Array.isArray(a)){if(s)return a=re(t,a,o,e);re(t,a,null,e)}else a==null||a===""||!t.firstChild?t.appendChild(e):t.replaceChild(e,t.firstChild);a=e}}return a}function Ne(t,e,a,o){let i=!1;for(let r=0,s=e.length;r<s;r++){let n=e[r],c=a&&a[t.length],m;if(!(n==null||n===!0||n===!1))if((m=typeof n)=="object"&&n.nodeType)t.push(n);else if(Array.isArray(n))i=Ne(t,n,c)||i;else if(m==="function")if(o){for(;typeof n=="function";)n=n();i=Ne(t,Array.isArray(n)?n:[n],Array.isArray(c)?c:[c])||i}else t.push(n),i=!0;else{const g=String(n);c&&c.nodeType===3&&c.data===g?t.push(c):t.push(document.createTextNode(g))}}return i}function it(t,e,a=null){for(let o=0,i=e.length;o<i;o++)t.insertBefore(e[o],a)}function re(t,e,a,o){if(a===void 0)return t.textContent="";const i=o||document.createTextNode("");if(e.length){let r=!1;for(let s=e.length-1;s>=0;s--){const n=e[s];if(i!==n){const c=n.parentNode===t;!r&&!s?c?t.replaceChild(i,n):t.insertBefore(i,a):c&&n.remove()}else r=!0}}else t.insertBefore(i,a);return[i]}const nt=[{id:"aiRnD",label:"AI R&D Investment",min:-5,max:5,default:0,description:"Global funding for AI research",category:"development"},{id:"computeGrowth",label:"Compute Scaling",min:-5,max:5,default:0,description:"GPU/TPU advancement rate",category:"development"},{id:"algorithmic",label:"Algorithmic Progress",min:-5,max:5,default:0,description:"Architecture breakthroughs",category:"development"},{id:"dataAvailability",label:"Data Availability",min:-3,max:3,default:0,description:"Training data quality & scale",category:"development"},{id:"geopolitical",label:"Geopolitical Race",min:-5,max:5,default:0,description:"US-China competition intensity",category:"competition"},{id:"talent",label:"Talent Concentration",min:-3,max:3,default:0,description:"Top researcher distribution",category:"competition"},{id:"corporateRace",label:"Corporate Competition",min:-5,max:5,default:0,description:"OpenAI/Google/Anthropic race",category:"competition"},{id:"energy",label:"Energy Infrastructure",min:-3,max:3,default:0,description:"Power & datacenter capacity",category:"competition"},{id:"safety",label:"AI Safety Research",min:-5,max:5,default:0,description:"Alignment R&D funding",category:"safety"},{id:"alignment",label:"Alignment Progress",min:-5,max:5,default:0,description:"Technical alignment breakthroughs",category:"safety"},{id:"regulation",label:"Government Regulation",min:-5,max:5,default:0,description:"Oversight & restrictions",category:"safety"},{id:"internationalCoord",label:"International Coordination",min:-5,max:5,default:0,description:"Global AI governance treaties",category:"safety"},{id:"economicPressure",label:"Economic Pressure",min:-3,max:3,default:0,description:"Market demand for AI",category:"wildcard"},{id:"publicSentiment",label:"Public Sentiment",min:-3,max:3,default:0,description:"Social acceptance level",category:"wildcard"},{id:"blackSwan",label:"Black Swan Events",min:-5,max:5,default:0,description:"Unexpected disruptions",category:"wildcard"}],He={development:{label:"Development",icon:"üöÄ",color:"#00f0ff"},competition:{label:"Competition",icon:"‚ö°",color:"#f97316"},safety:{label:"Safety & Governance",icon:"üõ°Ô∏è",color:"#22c55e"},wildcard:{label:"Wildcards",icon:"üé≤",color:"#8b5cf6"}},ze=[{id:"type0",kValue:.73,yRange:[0,.15],name:"Type 0",subtitle:"Pre-Planetary",description:"Current humanity (~0.73 K)",color:"#64748b",energy:"~10¬π¬≥ W"},{id:"preAGI",kValue:.85,yRange:[.15,.3],name:"Type 0.85",subtitle:"Advanced AI Era",description:"Narrow ‚Üí Reliable Agents",color:"#0ea5e9",energy:"~10¬π‚Å¥ W"},{id:"agi",kValue:.95,yRange:[.3,.5],name:"Type 0.95",subtitle:"AGI Threshold",description:"Human-level intelligence",color:"#00f0ff",energy:"~10¬π‚Åµ W"},{id:"super",kValue:1,yRange:[.5,.7],name:"Type I",subtitle:"Planetary Civilization",description:"Superintelligence era",color:"#22c55e",energy:"10¬π‚Å∂ W"},{id:"asi",kValue:1.5,yRange:[.7,.85],name:"Type I.5",subtitle:"ASI / Singularity",description:"Recursive self-improvement",color:"#8b5cf6",energy:"10¬≤‚Å∞ W"},{id:"post",kValue:2,yRange:[.85,1],name:"Type II+",subtitle:"Stellar / Beyond",description:"Dyson-sphere civilization",color:"#ec4899",energy:"10¬≤‚Å∂ W"}];ze.map((t,e)=>({level:e,range:`${t.yRange[0]} - ${t.yRange[1]}`,name:t.name,description:t.subtitle,color:t.color,yPos:(t.yRange[0]+t.yRange[1])/2}));const rt={utopian:{id:"utopian",title:"Aligned Flourishing",icon:"üåü",description:"Strong safety focus leads to beneficial superintelligence. Humanity flourishes.",conditions:"High safety + alignment + coordination",presets:{aiRnD:3,computeGrowth:2,algorithmic:2,dataAvailability:1,geopolitical:-2,talent:2,corporateRace:0,energy:2,safety:5,alignment:5,regulation:3,internationalCoord:5,economicPressure:1,publicSentiment:2,blackSwan:0}},race:{id:"race",title:"Competitive Race",icon:"üèÅ",description:"US-China competition drives rapid development. Outcome uncertain.",conditions:"High competition, moderate safety",presets:{aiRnD:4,computeGrowth:4,algorithmic:3,dataAvailability:2,geopolitical:5,talent:2,corporateRace:5,energy:2,safety:1,alignment:0,regulation:-2,internationalCoord:-3,economicPressure:3,publicSentiment:-1,blackSwan:0}},slowdown:{id:"slowdown",title:"Managed Transition",icon:"üõ°Ô∏è",description:"International agreements slow development. More time for alignment.",conditions:"High regulation + coordination",presets:{aiRnD:-1,computeGrowth:0,algorithmic:0,dataAvailability:0,geopolitical:-3,talent:0,corporateRace:-2,energy:0,safety:4,alignment:4,regulation:5,internationalCoord:5,economicPressure:-2,publicSentiment:3,blackSwan:0}},uncontrolled:{id:"uncontrolled",title:"Uncontrolled Takeoff",icon:"‚ö†Ô∏è",description:"Rapid development outpaces safety. High variance outcomes.",conditions:"High race + low safety",presets:{aiRnD:5,computeGrowth:5,algorithmic:5,dataAvailability:3,geopolitical:5,talent:3,corporateRace:5,energy:3,safety:-4,alignment:-3,regulation:-5,internationalCoord:-5,economicPressure:3,publicSentiment:-3,blackSwan:2}}},ot=[{id:"concepts",label:"Key Concepts",content:{title:"Understanding the Terms",paragraphs:['AGI, ASI and "superintelligence" are categories of capabilities an AI system can have. The singularity is a hypothesized event where self-improving AI drives progress so fast that human society becomes fundamentally unpredictable.'],bullets:["üß† AGI: AI matching human-level general intelligence across domains","üöÄ ASI / Superintelligence: AI vastly beyond humans in all cognitive areas","üåÄ Singularity: The moment when change becomes uncontrollable and irreversible","Key distinction: AGI and ASI describe what AI can do; Singularity is what happens to the world"]}},{id:"agi",label:"AGI",content:{title:"Artificial General Intelligence",paragraphs:["AGI refers to a system that can match or surpass human performance across a broad range of cognitive tasks, generalizing knowledge and solving novel problems without task-specific retraining."],bullets:["Transfers skills between domains, reasons abstractly","Adapts to new situations much like a human","Unlike current narrow AI, not limited to specific tasks","Often modeled as the precursor to ASI and singularity"]}},{id:"asi",label:"ASI",content:{title:"Artificial Superintelligence",paragraphs:['ASI, often called "superintelligence," is an intellect much smarter than the best human minds in virtually every domain‚Äîincluding scientific creativity, general wisdom, and social skills.'],bullets:["Surpasses human cognitive abilities in ALL important respects","Can autonomously improve its own capabilities","Typically assumed to arise from or after AGI","The regime where AI-driven change could become exponential"]}},{id:"singularity",label:"The Singularity",content:{title:"A Phase Change, Not a Capability",paragraphs:["The technological singularity is a hypothetical point when AI-driven technological growth becomes uncontrollable and irreversible, leading to profound and unpredictable changes in civilization."],bullets:['Tied to an "intelligence explosion" via positive feedback loops',"AI improving its own intelligence rapidly jumps to superintelligence","Existing social, economic, and political models break down as predictive tools","The macro-level transition that occurs when progress outpaces human comprehension"]}},{id:"sequence",label:"The Sequence",content:{title:"Order of Events",paragraphs:["In most mainstream narratives, the conceptual order is: Advanced Narrow AI ‚Üí AGI ‚Üí Intelligence Explosion ‚Üí ASI ‚Üí Singularity as the societal transition."],bullets:["‚ë† Advanced Narrow AI: Current LLMs, vision models, code models (where we are now)","‚ë° AGI: First broadly human-level system that can automate most intellectual work","‚ë¢ Intelligence Explosion: Recursive improvement compresses decades of R&D into months","‚ë£ ASI: Systems vastly beyond humans, emerging from rapid feedback loop","‚ë§ Singularity: Period where change is too rapid for traditional forecasting"]}},{id:"timelines",label:"Timelines",content:{title:"How Long Until Singularity?",paragraphs:["Expert surveys typically suggest AGI first, with uncertain gaps to superintelligence‚Äîranging from a couple of years to a few decades."],bullets:["Many timelines treat singularity as occurring shortly after AGI","If intelligence explosion happens, the gap could be weeks to months","Key uncertainty: Will recursive self-improvement actually work?","Median expert prediction for AGI: late 2020s to 2030s","Singularity often estimated within years to a few decades after AGI"]}},{id:"parameters",label:"Parameters",content:{title:"Understanding the Tweakable Parameters",paragraphs:["Each slider adjusts factors that accelerate (+) or delay (-) AI development. These represent the key variables experts consider when forecasting AI timelines."],categories:[{icon:"üöÄ",name:"Development Factors",color:"#00f0ff",items:[{name:"AI R&D Investment",desc:"Global funding for AI research‚Äîmore money = faster progress"},{name:"Compute Scaling",desc:"GPU/TPU hardware advancement‚Äîfaster chips = bigger models"},{name:"Algorithmic Progress",desc:"Architecture breakthroughs (like Transformers)‚Äîefficiency gains"},{name:"Data Availability",desc:"Quality and scale of training data‚Äîmore data = better models"}]},{icon:"‚ö°",name:"Competition Factors",color:"#f97316",items:[{name:"Geopolitical Race",desc:"US-China competition intensity‚Äîhigher = faster but riskier"},{name:"Talent Concentration",desc:"Where top researchers work‚Äîconcentration speeds progress"},{name:"Corporate Competition",desc:"OpenAI/Google/Anthropic race‚Äîcompetition accelerates timelines"},{name:"Energy Infrastructure",desc:"Power & datacenter capacity‚Äîenables larger training runs"}]},{icon:"üõ°Ô∏è",name:"Safety & Governance",color:"#22c55e",items:[{name:"AI Safety Research",desc:"Alignment R&D funding‚Äîmore = slower but safer development"},{name:"Alignment Progress",desc:"Technical alignment breakthroughs‚Äîincreases safety margin"},{name:"Government Regulation",desc:"Oversight & restrictions‚Äîcan slow or redirect development"},{name:"International Coordination",desc:"Global AI governance treaties‚Äîcoordination ‚Üí slower race"}]},{icon:"üé≤",name:"Wildcards",color:"#8b5cf6",items:[{name:"Economic Pressure",desc:"Market demand for AI‚Äîhigh demand accelerates deployment"},{name:"Public Sentiment",desc:"Social acceptance level‚Äîaffects regulation and funding"},{name:"Black Swan Events",desc:"Unexpected disruptions (wars, breakthroughs, disasters)"}]}]}}],st=[{year:1950,event:'Publication of "Computing Machinery and Intelligence"',detail:'Alan Turing published his seminal paper proposing the "imitation game" as a test for machine intelligence. Rather than asking "can machines think?", Turing reframed the question around observable behavior. The test involves a human evaluator attempting to distinguish between a machine and a human based solely on text conversation. This pragmatic approach sidestepped philosophical debates about consciousness and focused on functional equivalence.',personality:"Alan Turing",category:"foundation",implications:"The Turing test became the foundational benchmark for early AI research, establishing that intelligence should be measured by behavior rather than internal processes. This shifted decades of research toward building systems that could convincingly simulate human conversation, though critics later argued this encouraged shallow mimicry over genuine understanding. The paper also introduced concepts like machine learning and genetic algorithms that would become central to modern AI."},{year:1960,event:"Warning about automated system goals",detail:`Norbert Wiener, the father of cybernetics, published "Some Moral and Technical Consequences of Automation" in Science magazine. He warned that as machines become more powerful and autonomous, ensuring they operate according to human intentions becomes increasingly critical. Wiener emphasized that a system given a goal will pursue that goal without regard for unstated human values, potentially causing unintended harm. He compared this to the "monkey's paw" fable where wishes are granted literally but disastrously.`,personality:"Norbert Wiener",category:"safety",implications:`Wiener's prescient warnings anticipated the core alignment problem by over 50 years. His insight that powerful optimization systems can pursue goals in unexpected ways laid the intellectual groundwork for modern AI safety research. The "monkey's paw" analogy remains relevant today in discussions of specification gaming and reward hacking. His work established that technical capability alone is insufficient‚Äîsystems must be designed with human values in mind from the start.`},{year:1965,event:"Intelligence Explosion concept introduced",detail:'I. J. Good, a British mathematician and colleague of Turing, published "Speculations Concerning the First Ultraintelligent Machine." Good defined an ultraintelligent machine as one that could surpass any human intellectual activity and could therefore design even better machines. This recursive self-improvement, he argued, would lead to an "intelligence explosion" where capability accelerates beyond human comprehension. Good famously concluded this would be "the last invention that man need ever make."',personality:"I. J. Good",category:"singularity",implications:"Good's concept provided the theoretical foundation for discussions of AGI and superintelligence that continue today. The intelligence explosion hypothesis remains central to arguments about AI existential risk‚Äîif AI can improve itself, the gap between human and machine intelligence could grow rapidly and irreversibly. This paper directly influenced later thinkers like Vernor Vinge and Nick Bostrom and is still cited as the intellectual origin of singularity theory."},{year:1966,event:"ELIZA and PARRY chatbots",detail:'Joseph Weizenbaum at MIT created ELIZA, a program that simulated a Rogerian psychotherapist by using pattern matching and substitution rules. Users would type statements and ELIZA would respond with questions like "Why do you say that?" or reflections of their words. Despite having no understanding whatsoever, users found themselves emotionally engaged with the program. Weizenbaum was disturbed to find even colleagues who knew exactly how ELIZA worked still attributed understanding to it.',personality:"Joseph Weizenbaum, Kenneth Colby",category:"foundation",implications:`ELIZA revealed a profound truth about human psychology: we readily attribute intelligence and emotion to systems that merely mirror our words back to us. This "ELIZA effect" has major implications for modern AI‚Äîusers anthropomorphize chatbots, form relationships with them, and trust their outputs even when they shouldn't. Weizenbaum later became an AI critic, warning that society was too eager to grant machines authority over human affairs.`},{year:1983,event:"Technological Singularity concept",detail:'Vernor Vinge, a mathematician and science fiction author, presented his singularity ideas at an AAAI symposium and later refined them in his 1993 NASA paper "The Coming Technological Singularity." Vinge argued that superhuman intelligence would end the human era, creating changes so profound that pre-singularity humans could not comprehend them. He gave timelines predicting singularity between 2005 and 2030, driven by AI, brain-computer interfaces, or biological enhancement.',personality:"Vernor Vinge",category:"singularity",implications:"Vinge transformed the intelligence explosion from an academic curiosity into a cultural phenomenon. His framing of singularity as an event horizon‚Äîbeyond which prediction becomes impossible‚Äîcaptured the public imagination. The concept influenced Ray Kurzweil, spawned movements of singularitarians, and shaped how technologists and futurists think about long-term AI development. His timeline predictions, while not accurate in details, helped establish that superintelligence might arrive within human lifetimes."},{year:1990,event:"Loebner Prize established",detail:'Hugh Loebner, together with the Cambridge Center for Behavioral Studies, established an annual competition implementing the Turing test. The gold medal (never awarded) would go to a program indistinguishable from humans in unrestricted conversation with audio-visual input. Annual bronze medals went to the "most human-seeming" chatbot. Early winners used elaborate pattern matching, joke databases, and conversational tricks rather than genuine understanding.',personality:"Hugh Loebner",category:"foundation",implications:'The competition became controversial among AI researchers who felt it encouraged parlor tricks over genuine progress. Winners often succeeded through evasion, humor, or distracting judges rather than demonstrating understanding. This highlighted limitations of the Turing test as a research goal. The prize also sparked debates about what "passing" the test actually proves, foreshadowing modern discussions about whether LLMs that pass Turing-style evaluations are truly intelligent.'},{year:2e3,event:"SIAI founded (later MIRI)",detail:"Eliezer Yudkowsky founded the Singularity Institute for Artificial Intelligence (later renamed Machine Intelligence Research Institute) with the explicit goal of ensuring that advanced AI benefits humanity. Unlike other AI organizations focused on building capabilities, SIAI concentrated on the theoretical alignment problem‚Äîhow to ensure superintelligent AI has goals compatible with human flourishing. Yudkowsky had no traditional academic credentials but assembled rigorous arguments that attracted support from Peter Thiel and others.",personality:"Eliezer Yudkowsky",category:"safety",implications:"MIRI was the first organization to treat AGI alignment as a central technical challenge requiring dedicated research. It pioneered concepts like recursive self-improvement, goal stability, and value loading that became standard in AI safety discourse. Though critics questioned its unconventional approach, MIRI helped legitimize AI safety as a research field and trained many researchers who later joined DeepMind, OpenAI, and Anthropic. Its early warnings about AGI risk now seem prescient."},{year:2001,event:'"Creating Friendly AI" published',detail:"Eliezer Yudkowsky published a comprehensive document laying out the Friendly AI research program. The work argued that an AI system's goals matter enormously because capability without alignment leads to catastrophe. Friendly AI emphasized that goals should be stable under self-modification, that uncertainty about values should be preserved, and that the AI should defer to human oversight. The document introduced concepts later developed into coherent extrapolated volition.",personality:"Eliezer Yudkowsky",category:"alignment",implications:"This document established alignment as a distinct research agenda separate from capabilities. Its core insight‚Äîthat making AI friendly requires solving hard technical problems, not just instructing it to be nice‚Äîremains central to modern alignment research. Concepts like goal stability and value learning that Yudkowsky introduced are now studied by hundreds of researchers at major labs. The work also highlighted that alignment must be solved before creating superintelligence, not after."},{year:2005,event:"AAAI Machine Ethics Symposium",detail:"The American Association for Artificial Intelligence held the first Machine Ethics symposium, bringing together AI researchers, ethicists, and philosophers to examine the moral dimensions of increasingly autonomous systems. Papers covered topics from ethical decision-making in robots to the responsibilities of AI developers. This represented the emergence of machine ethics as a recognized subfield, distinct from but related to broader AI safety concerns.",personality:"AAAI",category:"safety",implications:"The symposium signaled that ethical considerations were becoming mainstream in AI research circles. It moved conversations about AI morality from science fiction and philosophy departments into engineering conferences. This laid groundwork for later efforts to incorporate ethics into AI development practices, from autonomous vehicle trolley problems to chatbot content policies. The field has since grown to encompass bias, fairness, accountability, and transparency."},{year:2006,event:"Future of Humanity Institute founded",detail:"Nick Bostrom and Anders Sandberg established the Future of Humanity Institute at Oxford University, creating an academic home for studying global catastrophic and existential risks. FHI treated AI risk alongside nuclear war, pandemics, and other threats to humanity's long-term survival. The institute brought rigorous academic methodology to questions previously explored mainly in informal contexts, publishing influential papers on probability of existential catastrophe and value of the future.",personality:"Nick Bostrom, Anders Sandberg",category:"safety",implications:"FHI legitimized AI risk research within mainstream academia, providing institutional support and credibility that attracted talented researchers. Bostrom's framing of AI as an existential risk helped galvanize funding and attention from both philanthropists and policymakers. The institute trained many who later led AI safety efforts elsewhere and produced research that influenced how labs design safety practices. Its interdisciplinary approach‚Äîcombining philosophy, mathematics, and policy‚Äîbecame a model for the field."},{year:2014,event:'"Superintelligence" published',detail:`Nick Bostrom's book "Superintelligence: Paths, Dangers, Strategies" provided the most comprehensive analysis yet of how advanced AI might emerge and the challenges of ensuring it remains beneficial. The book examined different paths to superintelligence (whole brain emulation, AI, biological enhancement), analyzed the "control problem" of keeping superintelligent systems aligned with human values, and explored potential strategies like boxing, oracles, and capability control. It became a New York Times bestseller.`,personality:"Nick Bostrom",category:"singularity",implications:'Superintelligence transformed AI safety from a fringe concern into a mainstream topic. Elon Musk, Bill Gates, and Stephen Hawking publicly cited the book when warning about AI risks. It provided vocabulary and frameworks that shaped subsequent discussions‚Äîconcepts like "instrumental convergence" and the "treacherous turn." The book directly influenced the creation of organizations like OpenAI and influenced safety programs at DeepMind. It remains the definitive introduction to AI existential risk.'},{year:2014,event:"Future of Life Institute founded",detail:"Max Tegmark, together with Jaan Tallinn and others, founded the Future of Life Institute to study existential risks from advanced technology, with AI as a primary focus. FLI bridged research and advocacy, convening scientists and public figures like Stephen Hawking and Morgan Freeman to raise awareness. The institute funded AI safety research grants and organized the Asilomar conference on beneficial AI.",personality:"Max Tegmark",category:"safety",implications:"FLI helped make AI safety a public policy issue by connecting researchers with celebrities, politicians, and philanthropists. Its 2017 Asilomar AI Principles, signed by thousands of researchers, established norms for beneficial AI development. The organization's AI safety research grants funded early work that built the foundation of the field. FLI demonstrated that concerns about advanced AI weren't limited to fringe groups but were shared by mainstream scientists and technologists."},{year:2015,event:"OpenAI founded",detail:"Sam Altman, Elon Musk, and other Silicon Valley figures launched OpenAI as a nonprofit AI research company with a $1 billion commitment. The founding charter explicitly stated the goal of ensuring artificial general intelligence benefits all of humanity. The organization was structured to prioritize safety over profits and promised to publish most research openly, countering concerns about AI development happening behind closed doors at large corporations.",personality:"Sam Altman, Elon Musk",category:"foundation",implications:"OpenAI embedded alignment into the charter of a major AI laboratory, signaling that safety could be a core mission rather than an afterthought. Its initial open approach (later modified) aimed to democratize AI research. The organization became a primary driver of capability advances with GPT models while maintaining safety teams. OpenAI's trajectory‚Äîfrom nonprofit to capped-profit, from open to more closed‚Äîreflects ongoing tensions between safety, competitive pressures, and commercial opportunities."},{year:2016,event:"AlphaGo defeats Lee Sedol",detail:"DeepMind's AlphaGo system defeated Lee Sedol, one of the world's top Go players, 4-1 in a highly publicized match. Go was considered a grand challenge for AI because its vast search space (more possible positions than atoms in the universe) seemed to require human-like intuition. AlphaGo combined deep neural networks with Monte Carlo tree search, learning from human games then surpassing human capability through self-play. Move 37 of Game 2 was particularly stunning‚Äîa move so unconventional that commentators initially thought it was a mistake.",personality:"DeepMind",category:"capability",implications:"AlphaGo demonstrated that AI could achieve superhuman performance in domains requiring creativity and intuition, not just brute-force calculation. The victory shocked both the AI research community and the broader public, advancing timelines for when human-level AI might emerge. The match crystallized concerns about the pace of AI progress‚Äîif Go could fall so quickly, what remained safe? Later versions (AlphaGo Zero, AlphaZero) learned entirely from self-play, suggesting less dependence on human knowledge."},{year:2016,event:`"AI Alignment: Why It's Hard and Where to Start"`,detail:'MIRI and Eliezer Yudkowsky published a comprehensive presentation explaining the core technical difficulties of AI alignment. The work decomposed the problem into subproblems: avoiding negative side effects, avoiding reward hacking, difficulty of specifying human values, and maintaining safety during self-improvement. It emphasized that alignment was not merely a matter of instructing AI to "be good" but required solving hard technical problems around goal specification and preservation.',personality:"Eliezer Yudkowsky, MIRI",category:"alignment",implications:"This document helped establish the intellectual framework for modern alignment research by clearly articulating what problems needed to be solved. Its taxonomy of alignment failures‚Äîside effects, reward hacking, specification gaming‚Äîprovided vocabulary for discussing problems that later emerged in deployed systems. The work influenced research agendas at newly-formed safety teams and helped new researchers understand why alignment was technically difficult rather than merely a matter of good intentions."},{year:2017,event:"Transformer architecture introduced",detail:'Researchers at Google Brain published "Attention Is All You Need," introducing the transformer architecture that would revolutionize AI. Unlike previous recurrent neural networks that processed sequences step-by-step, transformers used attention mechanisms to process entire sequences in parallel, enabling much larger models and faster training. The architecture made it practical to train models on unprecedented amounts of text data, leading directly to GPT, BERT, and subsequent large language models.',personality:"Vaswani et al., Google Brain",category:"capability",implications:"The transformer became the foundation of the LLM revolution, enabling the scaling that produced GPT-3, GPT-4, and other frontier models. Its architectural elegance allowed straightforward scaling‚Äîadd more parameters, more data, more compute‚Äîleading to the scaling laws that now drive AI development. The paper is the most cited AI paper in history and transformed the field by demonstrating that a single architecture could excel across vision, language, and multimodal tasks."},{year:2018,event:"BERT and RLHF mature",detail:"Google released BERT (Bidirectional Encoder Representations from Transformers), demonstrating the power of pre-training on large text corpora. Meanwhile, OpenAI advanced Reinforcement Learning from Human Feedback (RLHF), a technique for fine-tuning AI models based on human preferences. RLHF emerged as a practical method for aligning model behavior with human intentions by training on comparisons of outputs rather than explicit specifications of correct behavior.",personality:"Google, OpenAI",category:"alignment",implications:"RLHF became the dominant paradigm for making large language models more helpful and less harmful. It represented a significant practical advance in alignment‚Äîfinally, there was a technique that worked at scale. ChatGPT's success relied heavily on RLHF. However, RLHF has limitations: it can amplify biases in human feedback, be gamed by models, and may teach sycophancy rather than truthfulness. Understanding these limitations drives current alignment research on alternatives like constitutional AI."},{year:2020,event:"GPT-3 released",detail:"OpenAI released GPT-3, a 175-billion parameter language model that demonstrated emergent capabilities absent in smaller models. GPT-3 could perform tasks never explicitly trained for‚Äîtranslation, coding, arithmetic, even simple reasoning‚Äîsimply by being prompted with examples. The model showed that scaling could produce qualitatively new abilities, not just quantitative improvements. Access was provided through an API, sparking a wave of startup activity building on AI capabilities.",personality:"OpenAI",category:"capability",implications:"GPT-3 crystallized the scaling hypothesis‚Äîthat pushing more compute into larger models yields increasingly general capabilities. This reoriented the AI industry toward ever-larger models and helped initiate the current AI investment boom. The model also demonstrated concerning capabilities: generating convincing misinformation, producing subtly biased content, and occasionally synthesizing harmful information. These dual-use concerns intensified discussions of responsible deployment and spurred safety research."},{year:2022,event:"ChatGPT released",detail:"OpenAI released ChatGPT, a conversational AI fine-tuned with RLHF to be helpful, harmless, and honest. The interface‚Äîa simple chat window‚Äîmade advanced AI accessible to millions overnight. ChatGPT reached 100 million users within two months, the fastest-growing consumer application in history. Users found it could help with writing, coding, analysis, and creative tasks. It also occasionally hallucinated, made errors confidently, and could be jailbroken to produce harmful content.",personality:"OpenAI",category:"capability",implications:"ChatGPT brought AI capabilities and alignment challenges into mainstream awareness simultaneously. Suddenly, everyone from students to CEOs was using AI daily, experiencing both its remarkable abilities and its failures. The launch triggered an AI arms race among tech companies and massive investment in AI startups. It also made alignment concrete for the public‚Äîjailbreaks and hallucinations demonstrated in real-time why making AI systems behave beneficially requires ongoing work."},{year:2023,event:"First AI Safety Summit at Bletchley Park",detail:"The UK hosted the first global AI Safety Summit at Bletchley Park, the historic site where Turing worked on breaking the Enigma code. Representatives from 28 nations, including the US, China, and EU members, along with major AI companies signed the Bletchley Declaration acknowledging the need to collaborate on AI safety. The summit focused on frontier AI risks and established agreement on testing and evaluation frameworks.",personality:"UK Government, major AI labs",category:"safety",implications:"The summit marked AI safety's arrival as a major international policy priority, comparable to nuclear non-proliferation or climate negotiations. It was remarkable for including China alongside Western nations in discussions of AI governance. The commitments made, while not legally binding, established norms around safety testing and transparency. Follow-up summits in France and South Korea continued building international frameworks, suggesting AI governance may become a permanent feature of geopolitics."},{year:2024,event:"AI Safety Institutes established",detail:"The United States, United Kingdom, and several other nations established AI Safety Institutes to research and evaluate frontier AI systems. The US AISI operates under NIST, conducting technical evaluations of advanced models. The UK AISI partners directly with labs like OpenAI and Anthropic for pre-deployment testing. These institutes employ researchers from academia and industry to develop evaluation methodologies, red-teaming approaches, and safety standards.",personality:"CAIS, Redwood Research, government institutes",category:"safety",implications:"Government-backed AI safety institutes institutionalized oversight of frontier AI development. They represent a shift from industry self-regulation toward structured external review. These institutes are developing the frameworks that may eventually become mandatory requirements before deploying advanced systems. Their work on evaluation science‚Äîhow to test for dangerous capabilities like biological weapons design or autonomous hacking‚Äîis creating tools for responsible AI governance."},{year:2025,event:"Comprehensive AI alignment surveys",detail:"Major academic surveys consolidate the field of AI alignment, reviewing hundreds of papers and establishing a taxonomy of problems and approaches. Alignment is now recognized as a distinct technical discipline with its own conferences (AAAI AI Safety track), journals, and degree programs. Research covers technical approaches (RLHF, constitutional AI, interpretability, adversarial training) as well as governance and deployment practices.",personality:"Academia and industry researchers",category:"alignment",implications:`The formalization of alignment as an academic field ensures systematic progress and knowledge accumulation. Graduate programs now train researchers specifically in alignment, creating a pipeline of talent. Standard benchmarks allow comparison of approaches. The field's maturation also enables more precise discussions‚Äîrather than vague appeals to "safe AI," researchers can point to specific techniques addressing specific problems. This technical grounding is essential for building AI systems we can actually trust.`}],Ct={foundation:"#64748b",safety:"#22c55e",alignment:"#8b5cf6",singularity:"#f97316",capability:"#00f0ff"},Rt={aiRnD:{agi:.35,super:.4,asi:.45},computeGrowth:{agi:.3,super:.35,asi:.4},algorithmic:{agi:.45,super:.5,asi:.55},dataAvailability:{agi:.2,super:.25,asi:.25},geopolitical:{agi:.3,super:.35,asi:.35},talent:{agi:.2,super:.2,asi:.2},corporateRace:{agi:.35,super:.4,asi:.4},energy:{agi:.15,super:.2,asi:.25},safety:{agi:-.25,super:-.3,asi:-.35},alignment:{agi:-.2,super:-.35,asi:-.45},regulation:{agi:-.35,super:-.4,asi:-.45},internationalCoord:{agi:-.3,super:-.35,asi:-.4},economicPressure:{agi:.2,super:.25,asi:.25},publicSentiment:{agi:-.15,super:-.15,asi:-.15},blackSwan:{agi:.5,super:.6,asi:.7}},Gt=2027,Et=2035,Mt=2043,Lt=2025.95,Pt=2026.95;function Ht(t){(!t||typeof t!="object")&&(console.warn("Invalid levers object, using defaults"),t={});let e=0,a=0,o=0;for(const[n,c]of Object.entries(t)){const m=Rt[n];m&&typeof c=="number"&&!isNaN(c)&&(e+=c*m.agi,a+=c*m.super,o+=c*m.asi)}let i=Gt-e,r=Et-a,s=Mt-o;return i=Math.max(i,Pt),r=Math.max(r,i+1),s=Math.max(s,r+1),i=Math.round(i*10)/10,r=Math.round(r*10)/10,s=Math.round(s*10)/10,{agiYear:i,superYear:r,asiYear:s}}function Nt(t,e,a){return typeof t!="number"||typeof e!="number"||typeof a!="number"||isNaN(t)||isNaN(e)||isNaN(a)?(console.warn("Invalid timeline values for milestones"),[]):[{year:t,level:.45,label:"AGI",color:"#00f0ff"},{year:e,level:.65,label:"Superintelligence",color:"#8b5cf6"},{year:a,level:.85,label:"ASI",color:"#ec4899"}]}function zt(t){if(!t||typeof t!="object")return"race";const e=(t.safety||0)+(t.alignment||0)+(t.regulation||0)+(t.internationalCoord||0),a=(t.geopolitical||0)+(t.corporateRace||0),o=(t.aiRnD||0)+(t.computeGrowth||0)+(t.algorithmic||0);return e>=10&&a<=0?"slowdown":e>=6&&o>=4?"utopian":a>=6&&e<=0?"uncontrolled":"race"}function Yt(t){if(typeof t!="number"||isNaN(t))return"Invalid year";const e=Math.floor(t),a=t-e;return a<.25?`Early ${e}`:a<.5?`Mid ${e}`:a<.75?`Late ${e}`:`Late ${e}`}function Se(t){if(typeof t!="number"||isNaN(t))return"Unknown";const e=t-Lt;return e<1?"Less than 1 year":e<2?`~${Math.round(e*12)} months`:`~${Math.round(e)} years`}function ke(t,e){let a,o=0;return function(...i){const r=Date.now();r-o>e?(t.apply(this,i),o=r):(clearTimeout(a),a=setTimeout(()=>{t.apply(this,i),o=Date.now()},e-(r-o)))}}var Ot=A('<div class=share-dropdown><div class=share-header>Share your prediction</div><button class=share-option><div class=share-option-icon><svg width=20 height=20 viewBox="0 0 24 24"fill=currentColor><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"></path></svg></div><div class=share-option-content><div class=share-option-title>Share to X (Twitter)</div><div class=share-option-desc>Tag </div></div></button><button class=share-option><div class=share-option-icon><svg width=20 height=20 viewBox="0 0 24 24"fill=currentColor><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></div><div class=share-option-content><div class=share-option-title>Share to LinkedIn</div><div class=share-option-desc>Tag </div></div></button><div class=share-divider></div><button class=share-option><div class=share-option-icon><svg width=20 height=20 viewBox="0 0 24 24"fill=none stroke=currentColor stroke-width=2><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></div><div class=share-option-content><div class=share-option-title>Copy Link</div><div class=share-option-desc>Share directly'),Bt=A("<div class=share-backdrop>"),Dt=A('<div class=share-button-container><button class=share-btn title="Share your prediction"><svg width=20 height=20 viewBox="0 0 24 24"fill=none stroke=currentColor stroke-width=2><circle cx=18 cy=5 r=3></circle><circle cx=6 cy=12 r=3></circle><circle cx=18 cy=19 r=3></circle><line x1=8.59 y1=13.51 x2=15.42 y2=17.49></line><line x1=15.41 y1=6.51 x2=8.59 y2=10.49></line></svg>Share');function Ft(t){const[e,a]=Y(!1),o={twitter:"@faisalnair",linkedin:"faisalnazir"},i=()=>{const{agiYear:c,superYear:m,asiYear:g}=t.timeline,x=Math.round(g+3),C=t.scenario;return`My AI Timeline Prediction:

ü§ñ AGI: ${Math.round(c)}
üöÄ Superintelligence: ${Math.round(m)}
‚≠ê ASI: ${Math.round(g)}
üåÄ Singularity: ${Math.round(x)}

Scenario: ${C}

Predict your own timeline: ${typeof window<"u"?window.location.href:""}

#AI #Singularity #ArtificialIntelligence #Future #AGI #ASI

CC: ${o.twitter}`},r=()=>{const c=i(),m=`https://twitter.com/intent/tweet?text=${encodeURIComponent(c)}`;window.open(m,"_blank","width=600,height=400"),a(!1)},s=()=>{const c=window.location.href,m=`https://www.linkedin.com/sharing/share-offsite/?url=${encodeURIComponent(c)}`;window.open(m,"_blank","width=600,height=400"),a(!1)},n=async()=>{try{await navigator.clipboard.writeText(window.location.href),alert("Link copied to clipboard!"),a(!1)}catch(c){console.error("Failed to copy:",c)}};return(()=>{var c=Dt(),m=c.firstChild;return m.$$click=()=>a(!e()),h(c,w(ee,{get when(){return e()},get children(){var g=Ot(),x=g.firstChild,C=x.nextSibling,H=C.firstChild,te=H.nextSibling,ae=te.firstChild,E=ae.nextSibling;E.firstChild;var ie=C.nextSibling,oe=ie.firstChild,Te=oe.nextSibling,_e=Te.firstChild,me=_e.nextSibling;me.firstChild;var ge=ie.nextSibling,Ce=ge.nextSibling;return C.$$click=r,h(E,()=>o.twitter,null),ie.$$click=s,h(me,()=>o.linkedin,null),Ce.$$click=n,g}}),null),h(c,w(ee,{get when(){return e()},get children(){var g=Bt();return g.$$click=()=>a(!1),g}}),null),c})()}Ye(["click"]);var Ut=A("<div class=learn-overlay><div class=learn-header><div class=history-title><span class=icon>üìö</span><span>Understanding AI Futures</span></div><button class=close-btn>√ó</button></div><div class=learn-tabs></div><div class=learn-content>"),jt=A('<div class=app-container><header class=header-overlay><div class=logo-container><div class=logo-title>Singularity <span style=color:var(--color-primary)>Predictor</span></div><div class=logo-subtitle>Interactive Timeline</div></div><div class=controls-container><a href=https://ko-fi.com/X8X81PXBH2 target=_blank rel="noopener noreferrer"class=kofi-btn>‚òï Support</a><button class=about-btn>Start Learning</button></div></header><div class=predictions-bar><span class=pred-label>Predicted Timeline:</span><div class=pred-item><span class=pred-name>AGI</span><span class="pred-year agi"></span><span class=pred-time>(<!>)</span></div><div class=pred-item><span class=pred-name>Superintelligence</span><span class="pred-year super"></span><span class=pred-time>(<!>)</span></div><div class=pred-item><span class=pred-name>ASI</span><span class="pred-year asi"></span><span class=pred-time>(<!>)</span></div><div class="pred-item singularity"><span class=pred-name>Singularity</span><span class="pred-year singularity-val"></span><span class=pred-time>(<!>)</span></div></div><div class=graph-fullscreen><svg></svg></div><div class=lever-panel><div class=lever-categories-grid></div><button class=reset-btn>‚Ü∫ Reset</button></div><div class=scenarios-panel><div class=scenario-title>Scenarios'),Wt=A("<div class=lever-category><div class=category-header><span class=category-icon></span><span class=category-label>"),Vt=A("<div class=lever><div class=lever-header><span class=lever-label></span><span></span></div><input type=range class=lever-slider step=1>"),qt=A("<div class=scenario-expanded><div class=scenario-desc></div><div class=scenario-conditions>"),Kt=A("<div><div class=scenario-header><span class=scenario-icon></span><span class=scenario-name>"),Zt=A("<button>"),Xt=A("<ul>"),Jt=A("<div class=params-grid>"),Qt=A("<div class=learn-card><h3>"),ea=A("<p>"),ta=A("<li>"),aa=A("<div class=param-category><div class=param-category-header><span class=param-icon></span><span class=param-cat-name></span></div><div class=param-items>"),ia=A("<div class=param-item><div class=param-name></div><div class=param-desc>");function na(){const[t,e]=Y({...rt.race.presets}),[a,o]=Y({agiYear:2027,superYear:2035,asiYear:2043}),[i,r]=Y("race"),[s,n]=Y(!1),[c,m]=Y("singularity"),[g,x]=Y("race"),[C]=Y(!1),[H]=Y("all"),[te,ae]=Y(null);let E,ie=!1;const oe=pe(()=>{const d=t();return Ht(d)}),Te=pe(()=>zt(t())),_e=pe(()=>{const d=oe();return Ce(d)}),me=pe(()=>{const d=oe();return Nt(d.agiYear,d.superYear,d.asiYear)});Me(()=>{const d=oe();o(d),r(Te()),ie&&Be(d)}),vt(()=>{Oe(),window.addEventListener("resize",ke(ge,150))}),wt(()=>{window.removeEventListener("resize",ke(ge,150))});const ge=ke(()=>{E&&Oe()},150);function Ce(d){const f=d.asiYear+3,S=[],v=.12,$=.95;for(let b=2025.95;b<=f;b+=.3){const I=(b-2025.95)/(f-2025.95),k=v+($-v)*I*I;S.push({x:b,y:Math.min(k,.99)})}return S}function Re(d){return d+3}function Oe(){if(!E)return;const d=E.parentElement,u=d.clientWidth,f=d.clientHeight,S=u<768,v=u>=768&&u<1024,$=S?{top:40,right:20,bottom:60,left:60}:v?{top:50,right:40,bottom:65,left:80}:{top:60,right:40,bottom:70,left:240},b=Math.max(100,u-$.left-$.right),I=Math.max(100,f-$.top-$.bottom);ue(E).selectAll("*").remove();const k=ue(E).attr("width",u).attr("height",f),N=k.append("defs"),R=N.append("linearGradient").attr("id","trajGradient").attr("gradientUnits","userSpaceOnUse");R.append("stop").attr("offset","0%").attr("stop-color","#22c55e"),R.append("stop").attr("offset","30%").attr("stop-color","#00f0ff"),R.append("stop").attr("offset","60%").attr("stop-color","#8b5cf6"),R.append("stop").attr("offset","100%").attr("stop-color","#ec4899");const M=N.append("filter").attr("id","glow").attr("x","-50%").attr("y","-50%").attr("width","200%").attr("height","200%");M.append("feGaussianBlur").attr("stdDeviation","4").attr("result","blur"),M.append("feMerge").selectAll("feMergeNode").data(["blur","SourceGraphic"]).enter().append("feMergeNode").attr("in",O=>O),k.append("rect").attr("class","graph-bg").attr("width",u).attr("height",f);const p=k.append("g").attr("transform",`translate(${$.left},${$.top})`);k.node().__graphData={g:p,innerWidth:b,innerHeight:I,margin:$},ie=!0,Be(a())}function Be(d){if(!E||!E.__graphData)return;const{g:u,innerWidth:f,innerHeight:S}=ue(E).node().__graphData,v=_e(),$=me(),b=2024,I=Math.ceil(d.asiYear+8),k=v.map(l=>l.y),N=0,R=Math.min(1,Math.max(...k)+.1),M=xe().domain([b,I]).range([0,f]),p=xe().domain([N,R]).range([S,0]);u.selectAll("*").remove();const O=u.append("g").attr("class","grid");for(let l=Math.ceil(b/5)*5;l<=I;l+=5)O.append("line").attr("x1",M(l)).attr("y1",0).attr("x2",M(l)).attr("y2",S).attr("stroke","rgba(255,255,255,0.04)").attr("stroke-width",1);ze.forEach((l,F)=>{if(l.yRange[0]>R)return;const T=Math.max(l.yRange[0],N),Z=Math.min(l.yRange[1],R);if(T>=Z)return;const U=p(Z),j=p(T),ve=Math.max(0,j-U);ve<=0||f<=0||(u.append("rect").attr("x",0).attr("y",U).attr("width",Math.max(0,f)).attr("height",ve).attr("fill",l.color).attr("opacity",.08),l.yRange[0]>N&&l.yRange[0]<=R&&u.append("line").attr("x1",0).attr("y1",p(l.yRange[0])).attr("x2",f).attr("y2",p(l.yRange[0])).attr("stroke",l.color).attr("stroke-width",1).attr("opacity",.2))});const q=u.append("g").attr("transform",`translate(0,${S})`);for(let l=Math.ceil(b/5)*5;l<=I;l+=5)q.append("text").attr("x",M(l)).attr("y",30).attr("text-anchor","middle").attr("fill","#64748b").attr("font-size","11px").attr("font-family","Space Grotesk").text(l);q.append("line").attr("x1",0).attr("y1",0).attr("x2",f).attr("y2",0).attr("stroke","rgba(255,255,255,0.1)").attr("stroke-width",1),ze.forEach(l=>{const F=(l.yRange[0]+l.yRange[1])/2,T=p(F);T<0||T>S||(u.append("text").attr("x",10).attr("y",T).attr("text-anchor","start").attr("fill","#94a3b8").attr("font-size","14px").attr("font-weight","700").attr("font-family","Space Grotesk").style("text-shadow","0 2px 4px rgba(0,0,0,0.8)").text(l.name),u.append("text").attr("x",10).attr("y",T+14).attr("text-anchor","start").attr("fill","#64748b").attr("font-size","10px").attr("font-family","Space Grotesk").style("text-shadow","0 2px 4px rgba(0,0,0,0.8)").text(l.subtitle))}),u.append("text").attr("x",10).attr("y",20).attr("fill","#64748b").attr("font-size","11px").attr("font-weight","600").attr("font-family","Space Grotesk").text("KARDASHEV SCALE");const se=M(2025.95),le=p(.12);u.append("circle").attr("cx",se).attr("cy",le).attr("r",8).attr("fill","#22c55e").style("filter","url(#glow)"),u.append("text").attr("x",se).attr("y",le-20).attr("text-anchor","middle").attr("fill","#22c55e").attr("font-size","14px").attr("font-weight","700").attr("font-family","Space Grotesk").text("NOW");const fe=Qe().x(l=>M(l.x)).y(l=>p(l.y)).curve(et.alpha(.5)),ye=It().x(l=>M(l.x)).y0(S).y1(l=>p(l.y)).curve(et.alpha(.5));u.append("path").attr("class","trajectory-area").attr("d",ye(v)).attr("fill","url(#trajGradient)").attr("opacity",.08),u.append("path").attr("class","trajectory-glow").attr("d",fe(v)).attr("fill","none").attr("stroke","#00f0ff").attr("stroke-width",12).attr("opacity",.15).style("filter","blur(6px)"),u.append("path").attr("class","trajectory-path").attr("d",fe(v)).attr("fill","none").attr("stroke","url(#trajGradient)").attr("stroke-width",3).attr("stroke-linecap","round");const be=Re(d.asiYear);function Ge(l){let F=v[0];for(const T of v)Math.abs(T.x-l)<Math.abs(F.x-l)&&(F=T);return F.y}[...$,{year:be,label:"Singularity",color:"#ec4899"}].forEach((l,F)=>{if(l.year<b||l.year>I)return;const T=M(l.year),Z=Ge(l.year),U=p(Z),j=u.append("g").attr("class","milestone-group").attr("transform",`translate(${T},0)`);j.append("line").attr("x1",0).attr("y1",U).attr("x2",0).attr("y2",S).attr("stroke",l.color).attr("stroke-width",1).attr("stroke-dasharray","4,4").attr("opacity",.5),j.append("circle").attr("cy",U).attr("r",10).attr("fill",l.color).style("filter","url(#glow)"),j.append("text").attr("x",0).attr("y",U-45).attr("text-anchor","middle").attr("fill",l.color).attr("font-size","16px").attr("font-weight","600").attr("font-family","Space Grotesk").text(l.label),j.append("text").attr("x",0).attr("y",U-25).attr("text-anchor","middle").attr("fill",l.color).attr("font-size","18px").attr("font-weight","700").attr("font-family","Space Grotesk").text(Yt(l.year))})}function De(){if(!timelineSvgRef)return;const d=2500,u=280,f={top:40,right:50,bottom:60,left:50},S=d-f.left-f.right,v=u-f.top-f.bottom;ue(timelineSvgRef).selectAll("*").remove();const b=ue(timelineSvgRef).attr("width",d).attr("height",u).append("g").attr("transform",`translate(${f.left},${f.top})`),I=xe().domain([1950,2025]).range([0,S]),k=xe().domain([0,1]).range([v,0]),N=[];for(let p=1950;p<=2025;p+=1){const O=(p-1950)/75,q=.02+.1*Math.pow(O,.4);N.push({x:p,y:q})}const R=Qe().x(p=>I(p.x)).y(p=>k(p.y)).curve(xt);b.append("path").attr("d",R(N)).attr("fill","none").attr("stroke","#64748b").attr("stroke-width",2).attr("opacity",.6),(H()==="all"?st:st.filter(p=>p.category===H())).forEach(p=>{const O=(p.year-1950)/75,q=.02+.1*Math.pow(O,.4);b.append("circle").attr("cx",I(p.year)).attr("cy",k(q)).attr("r",6).attr("fill",Ct[p.category]).attr("stroke","#0a0a12").attr("stroke-width",2).style("cursor","pointer").on("mouseover",()=>ae(p)).on("mouseout",()=>ae(null)),b.append("text").attr("x",I(p.year)).attr("y",v+20).attr("text-anchor","middle").attr("fill","#64748b").attr("font-size","9px").attr("font-family","Space Grotesk").text(p.year)}),b.append("line").attr("x1",0).attr("y1",v).attr("x2",S).attr("y2",v).attr("stroke","rgba(255,255,255,0.2)").attr("stroke-width",1);for(let p=1950;p<=2020;p+=10)b.append("text").attr("x",I(p)).attr("y",v+40).attr("text-anchor","middle").attr("fill","#94a3b8").attr("font-size","12px").attr("font-weight","600").attr("font-family","Space Grotesk").text(p);b.append("circle").attr("cx",I(2025)).attr("cy",k(.12)).attr("r",10).attr("fill","#22c55e").style("filter","drop-shadow(0 0 8px #22c55e)"),b.append("text").attr("x",I(2025)).attr("y",k(.12)-18).attr("text-anchor","middle").attr("fill","#22c55e").attr("font-size","12px").attr("font-weight","700").attr("font-family","Space Grotesk").text("NOW")}Me(()=>{C()&&setTimeout(()=>De(),50)}),Me(()=>{H(),C()&&timelineSvgRef&&De()});const lt=ke((d,u)=>{e(f=>({...f,[d]:parseFloat(u)}))},50);function Fe(){e(nt.reduce((d,u)=>(d[u.id]=u.default,d),{})),x(null)}function ct(d){return d>0?"positive":d<0?"negative":"neutral"}function dt(d){return d>0?`+${d}`:d.toString()}const ht=()=>{const d={};return nt.forEach(u=>{d[u.category]||(d[u.category]=[]),d[u.category].push(u)}),d};function ut(d){g()===d.id?Fe():(x(d.id),d.presets&&e({...d.presets}))}return(()=>{var d=jt(),u=d.firstChild,f=u.firstChild,S=f.firstChild,v=S.firstChild;v.nextSibling;var $=f.nextSibling,b=$.firstChild,I=b.nextSibling,k=u.nextSibling,N=k.firstChild,R=N.nextSibling,M=R.firstChild,p=M.nextSibling,O=p.nextSibling,q=O.firstChild,se=q.nextSibling;se.nextSibling;var le=R.nextSibling,fe=le.firstChild,ye=fe.nextSibling,be=ye.nextSibling,Ge=be.firstChild,Ee=Ge.nextSibling;Ee.nextSibling;var l=le.nextSibling,F=l.firstChild,T=F.nextSibling,Z=T.nextSibling,U=Z.firstChild,j=U.nextSibling;j.nextSibling;var ve=l.nextSibling,pt=ve.firstChild,Ue=pt.nextSibling,je=Ue.nextSibling,mt=je.firstChild,We=mt.nextSibling;We.nextSibling;var Ve=k.nextSibling,qe=Ve.firstChild,Ke=Ve.nextSibling,Ze=Ke.firstChild,gt=Ze.nextSibling,Xe=Ke.nextSibling;Xe.firstChild,h($,w(Ft,{get timeline(){return a()},get scenario(){return i()}}),b),I.$$click=()=>n(!s()),h(p,()=>Math.round(a().agiYear)),h(O,()=>Se(a().agiYear),se),h(ye,()=>Math.round(a().superYear)),h(be,()=>Se(a().superYear),Ee),h(T,()=>Math.round(a().asiYear)),h(Z,()=>Se(a().asiYear),j),h(Ue,()=>Math.round(Re(a().asiYear))),h(je,()=>Se(Re(a().asiYear)),We);var Je=E;return typeof Je=="function"?Tt(Je,qe):E=qe,h(Ze,w(W,{get each(){return Object.entries(ht())},children:([G,B])=>(()=>{var X=Wt(),J=X.firstChild,Q=J.firstChild,ne=Q.nextSibling;return h(Q,()=>He[G].icon),h(ne,()=>He[G].label),h(X,w(W,{each:B,children:y=>(()=>{var _=Vt(),we=_.firstChild,D=we.firstChild,L=D.nextSibling,z=we.nextSibling;return h(D,()=>y.label),h(L,()=>dt(t()[y.id])),z.$$input=P=>lt(y.id,P.target.value),V(P=>{var ce=`lever-value ${ct(t()[y.id])}`,Ae=y.min,Ie=y.max;return ce!==P.e&&Le(L,P.e=ce),Ae!==P.t&&at(z,"min",P.t=Ae),Ie!==P.a&&at(z,"max",P.a=Ie),P},{e:void 0,t:void 0,a:void 0}),V(()=>z.value=t()[y.id]),_})()}),null),V(y=>Pe(J,"color",He[G].color)),X})()})),gt.$$click=Fe,h(Xe,w(W,{get each(){return Object.values(rt)},children:G=>(()=>{var B=Kt(),X=B.firstChild,J=X.firstChild,Q=J.nextSibling;return B.$$click=()=>ut(G),h(J,()=>G.icon),h(Q,()=>G.title),h(B,w(ee,{get when(){return g()===G.id},get children(){var ne=qt(),y=ne.firstChild,_=y.nextSibling;return h(y,()=>G.description),h(_,()=>G.conditions),ne}}),null),V(()=>Le(B,`scenario-card ${g()===G.id?"active":""}`)),B})()}),null),h(d,w(ee,{get when(){return s()},get children(){var G=Ut(),B=G.firstChild,X=B.firstChild,J=X.nextSibling,Q=B.nextSibling,ne=Q.nextSibling;return J.$$click=()=>n(!1),h(Q,w(W,{each:ot,children:y=>(()=>{var _=Zt();return _.$$click=()=>m(y.id),h(_,()=>y.label),V(()=>Le(_,`learn-tab ${c()===y.id?"active":""}`)),_})()})),h(ne,w(W,{each:ot,children:y=>w(ee,{get when(){return c()===y.id},get children(){var _=Qt(),we=_.firstChild;return h(we,()=>y.content.title),h(_,w(W,{get each(){return y.content.paragraphs},children:D=>(()=>{var L=ea();return h(L,D),L})()}),null),h(_,w(ee,{get when(){return y.content.bullets},get children(){var D=Xt();return h(D,w(W,{get each(){return y.content.bullets},children:L=>(()=>{var z=ta();return h(z,L),z})()})),D}}),null),h(_,w(ee,{get when(){return y.content.categories},get children(){var D=Jt();return h(D,w(W,{get each(){return y.content.categories},children:L=>(()=>{var z=aa(),P=z.firstChild,ce=P.firstChild,Ae=ce.nextSibling,Ie=P.nextSibling;return h(ce,()=>L.icon),h(Ae,()=>L.name),h(Ie,w(W,{get each(){return L.items},children:K=>(()=>{var de=ia(),he=de.firstChild,ft=he.nextSibling;return h(he,()=>K.name),h(ft,()=>K.desc),de})()})),V(K=>{var de=L.color,he=L.color;return de!==K.e&&Pe(z,"border-color",K.e=de),he!==K.t&&Pe(P,"color",K.t=he),K},{e:void 0,t:void 0}),z})()})),D}}),null),_}})})),G}}),null),d})()}Ye(["click","input"]);var ra=A('<div class=error-boundary><div class=error-container><div class=error-icon>‚ö†Ô∏è</div><h2 class=error-title>Something went wrong</h2><p class=error-message></p><div class=error-actions><button class=error-btn>Try Again</button><button class="error-btn secondary">Reload Page'),oa=A("<details class=error-details><summary>Error Details</summary><pre class=error-stack>");function sa(t){const[e,a]=Y(null);At(i=>{console.error("Error caught by boundary:",i),a(i)});const o=()=>{a(null)};return e()?(()=>{var i=ra(),r=i.firstChild,s=r.firstChild,n=s.nextSibling,c=n.nextSibling,m=c.nextSibling,g=m.firstChild,x=g.nextSibling;return h(c,()=>e().message||"An unexpected error occurred"),g.$$click=o,x.$$click=()=>window.location.reload(),h(r,(()=>{var C=St(()=>!1);return()=>C()&&(()=>{var H=oa(),te=H.firstChild,ae=te.nextSibling;return h(ae,()=>e().stack||"No stack trace available"),H})()})(),null),i})():t.children}Ye(["click"]);const la=document.getElementById("root");$t(()=>w(sa,{get children(){return w(na,{})}}),la);
