/**
 * AI History Timeline Data
 * Key milestones in AI, AI Safety, Alignment, and Singularity research
 */

export const AI_HISTORY = [
  {
    year: 1950,
    event: 'Publication of "Computing Machinery and Intelligence"',
    detail: 'Alan Turing published his seminal paper proposing the "imitation game" as a test for machine intelligence. Rather than asking "can machines think?", Turing reframed the question around observable behavior. The test involves a human evaluator attempting to distinguish between a machine and a human based solely on text conversation. This pragmatic approach sidestepped philosophical debates about consciousness and focused on functional equivalence.',
    personality: 'Alan Turing',
    category: 'foundation',
    implications: 'The Turing test became the foundational benchmark for early AI research, establishing that intelligence should be measured by behavior rather than internal processes. This shifted decades of research toward building systems that could convincingly simulate human conversation, though critics later argued this encouraged shallow mimicry over genuine understanding. The paper also introduced concepts like machine learning and genetic algorithms that would become central to modern AI.',
  },
  {
    year: 1960,
    event: 'Warning about automated system goals',
    detail: 'Norbert Wiener, the father of cybernetics, published "Some Moral and Technical Consequences of Automation" in Science magazine. He warned that as machines become more powerful and autonomous, ensuring they operate according to human intentions becomes increasingly critical. Wiener emphasized that a system given a goal will pursue that goal without regard for unstated human values, potentially causing unintended harm. He compared this to the "monkey\'s paw" fable where wishes are granted literally but disastrously.',
    personality: 'Norbert Wiener',
    category: 'safety',
    implications: 'Wiener\'s prescient warnings anticipated the core alignment problem by over 50 years. His insight that powerful optimization systems can pursue goals in unexpected ways laid the intellectual groundwork for modern AI safety research. The "monkey\'s paw" analogy remains relevant today in discussions of specification gaming and reward hacking. His work established that technical capability alone is insufficient—systems must be designed with human values in mind from the start.',
  },
  {
    year: 1965,
    event: 'Intelligence Explosion concept introduced',
    detail: 'I. J. Good, a British mathematician and colleague of Turing, published "Speculations Concerning the First Ultraintelligent Machine." Good defined an ultraintelligent machine as one that could surpass any human intellectual activity and could therefore design even better machines. This recursive self-improvement, he argued, would lead to an "intelligence explosion" where capability accelerates beyond human comprehension. Good famously concluded this would be "the last invention that man need ever make."',
    personality: 'I. J. Good',
    category: 'singularity',
    implications: 'Good\'s concept provided the theoretical foundation for discussions of AGI and superintelligence that continue today. The intelligence explosion hypothesis remains central to arguments about AI existential risk—if AI can improve itself, the gap between human and machine intelligence could grow rapidly and irreversibly. This paper directly influenced later thinkers like Vernor Vinge and Nick Bostrom and is still cited as the intellectual origin of singularity theory.',
  },
  {
    year: 1966,
    event: 'ELIZA and PARRY chatbots',
    detail: 'Joseph Weizenbaum at MIT created ELIZA, a program that simulated a Rogerian psychotherapist by using pattern matching and substitution rules. Users would type statements and ELIZA would respond with questions like "Why do you say that?" or reflections of their words. Despite having no understanding whatsoever, users found themselves emotionally engaged with the program. Weizenbaum was disturbed to find even colleagues who knew exactly how ELIZA worked still attributed understanding to it.',
    personality: 'Joseph Weizenbaum, Kenneth Colby',
    category: 'foundation',
    implications: 'ELIZA revealed a profound truth about human psychology: we readily attribute intelligence and emotion to systems that merely mirror our words back to us. This "ELIZA effect" has major implications for modern AI—users anthropomorphize chatbots, form relationships with them, and trust their outputs even when they shouldn\'t. Weizenbaum later became an AI critic, warning that society was too eager to grant machines authority over human affairs.',
  },
  {
    year: 1983,
    event: 'Technological Singularity concept',
    detail: 'Vernor Vinge, a mathematician and science fiction author, presented his singularity ideas at an AAAI symposium and later refined them in his 1993 NASA paper "The Coming Technological Singularity." Vinge argued that superhuman intelligence would end the human era, creating changes so profound that pre-singularity humans could not comprehend them. He gave timelines predicting singularity between 2005 and 2030, driven by AI, brain-computer interfaces, or biological enhancement.',
    personality: 'Vernor Vinge',
    category: 'singularity',
    implications: 'Vinge transformed the intelligence explosion from an academic curiosity into a cultural phenomenon. His framing of singularity as an event horizon—beyond which prediction becomes impossible—captured the public imagination. The concept influenced Ray Kurzweil, spawned movements of singularitarians, and shaped how technologists and futurists think about long-term AI development. His timeline predictions, while not accurate in details, helped establish that superintelligence might arrive within human lifetimes.',
  },
  {
    year: 1990,
    event: 'Loebner Prize established',
    detail: 'Hugh Loebner, together with the Cambridge Center for Behavioral Studies, established an annual competition implementing the Turing test. The gold medal (never awarded) would go to a program indistinguishable from humans in unrestricted conversation with audio-visual input. Annual bronze medals went to the "most human-seeming" chatbot. Early winners used elaborate pattern matching, joke databases, and conversational tricks rather than genuine understanding.',
    personality: 'Hugh Loebner',
    category: 'foundation',
    implications: 'The competition became controversial among AI researchers who felt it encouraged parlor tricks over genuine progress. Winners often succeeded through evasion, humor, or distracting judges rather than demonstrating understanding. This highlighted limitations of the Turing test as a research goal. The prize also sparked debates about what "passing" the test actually proves, foreshadowing modern discussions about whether LLMs that pass Turing-style evaluations are truly intelligent.',
  },
  {
    year: 2000,
    event: 'SIAI founded (later MIRI)',
    detail: 'Eliezer Yudkowsky founded the Singularity Institute for Artificial Intelligence (later renamed Machine Intelligence Research Institute) with the explicit goal of ensuring that advanced AI benefits humanity. Unlike other AI organizations focused on building capabilities, SIAI concentrated on the theoretical alignment problem—how to ensure superintelligent AI has goals compatible with human flourishing. Yudkowsky had no traditional academic credentials but assembled rigorous arguments that attracted support from Peter Thiel and others.',
    personality: 'Eliezer Yudkowsky',
    category: 'safety',
    implications: 'MIRI was the first organization to treat AGI alignment as a central technical challenge requiring dedicated research. It pioneered concepts like recursive self-improvement, goal stability, and value loading that became standard in AI safety discourse. Though critics questioned its unconventional approach, MIRI helped legitimize AI safety as a research field and trained many researchers who later joined DeepMind, OpenAI, and Anthropic. Its early warnings about AGI risk now seem prescient.',
  },
  {
    year: 2001,
    event: '"Creating Friendly AI" published',
    detail: 'Eliezer Yudkowsky published a comprehensive document laying out the Friendly AI research program. The work argued that an AI system\'s goals matter enormously because capability without alignment leads to catastrophe. Friendly AI emphasized that goals should be stable under self-modification, that uncertainty about values should be preserved, and that the AI should defer to human oversight. The document introduced concepts later developed into coherent extrapolated volition.',
    personality: 'Eliezer Yudkowsky',
    category: 'alignment',
    implications: 'This document established alignment as a distinct research agenda separate from capabilities. Its core insight—that making AI friendly requires solving hard technical problems, not just instructing it to be nice—remains central to modern alignment research. Concepts like goal stability and value learning that Yudkowsky introduced are now studied by hundreds of researchers at major labs. The work also highlighted that alignment must be solved before creating superintelligence, not after.',
  },
  {
    year: 2005,
    event: 'AAAI Machine Ethics Symposium',
    detail: 'The American Association for Artificial Intelligence held the first Machine Ethics symposium, bringing together AI researchers, ethicists, and philosophers to examine the moral dimensions of increasingly autonomous systems. Papers covered topics from ethical decision-making in robots to the responsibilities of AI developers. This represented the emergence of machine ethics as a recognized subfield, distinct from but related to broader AI safety concerns.',
    personality: 'AAAI',
    category: 'safety',
    implications: 'The symposium signaled that ethical considerations were becoming mainstream in AI research circles. It moved conversations about AI morality from science fiction and philosophy departments into engineering conferences. This laid groundwork for later efforts to incorporate ethics into AI development practices, from autonomous vehicle trolley problems to chatbot content policies. The field has since grown to encompass bias, fairness, accountability, and transparency.',
  },
  {
    year: 2006,
    event: 'Future of Humanity Institute founded',
    detail: 'Nick Bostrom and Anders Sandberg established the Future of Humanity Institute at Oxford University, creating an academic home for studying global catastrophic and existential risks. FHI treated AI risk alongside nuclear war, pandemics, and other threats to humanity\'s long-term survival. The institute brought rigorous academic methodology to questions previously explored mainly in informal contexts, publishing influential papers on probability of existential catastrophe and value of the future.',
    personality: 'Nick Bostrom, Anders Sandberg',
    category: 'safety',
    implications: 'FHI legitimized AI risk research within mainstream academia, providing institutional support and credibility that attracted talented researchers. Bostrom\'s framing of AI as an existential risk helped galvanize funding and attention from both philanthropists and policymakers. The institute trained many who later led AI safety efforts elsewhere and produced research that influenced how labs design safety practices. Its interdisciplinary approach—combining philosophy, mathematics, and policy—became a model for the field.',
  },
  {
    year: 2014,
    event: '"Superintelligence" published',
    detail: 'Nick Bostrom\'s book "Superintelligence: Paths, Dangers, Strategies" provided the most comprehensive analysis yet of how advanced AI might emerge and the challenges of ensuring it remains beneficial. The book examined different paths to superintelligence (whole brain emulation, AI, biological enhancement), analyzed the "control problem" of keeping superintelligent systems aligned with human values, and explored potential strategies like boxing, oracles, and capability control. It became a New York Times bestseller.',
    personality: 'Nick Bostrom',
    category: 'singularity',
    implications: 'Superintelligence transformed AI safety from a fringe concern into a mainstream topic. Elon Musk, Bill Gates, and Stephen Hawking publicly cited the book when warning about AI risks. It provided vocabulary and frameworks that shaped subsequent discussions—concepts like "instrumental convergence" and the "treacherous turn." The book directly influenced the creation of organizations like OpenAI and influenced safety programs at DeepMind. It remains the definitive introduction to AI existential risk.',
  },
  {
    year: 2014,
    event: 'Future of Life Institute founded',
    detail: 'Max Tegmark, together with Jaan Tallinn and others, founded the Future of Life Institute to study existential risks from advanced technology, with AI as a primary focus. FLI bridged research and advocacy, convening scientists and public figures like Stephen Hawking and Morgan Freeman to raise awareness. The institute funded AI safety research grants and organized the Asilomar conference on beneficial AI.',
    personality: 'Max Tegmark',
    category: 'safety',
    implications: 'FLI helped make AI safety a public policy issue by connecting researchers with celebrities, politicians, and philanthropists. Its 2017 Asilomar AI Principles, signed by thousands of researchers, established norms for beneficial AI development. The organization\'s AI safety research grants funded early work that built the foundation of the field. FLI demonstrated that concerns about advanced AI weren\'t limited to fringe groups but were shared by mainstream scientists and technologists.',
  },
  {
    year: 2015,
    event: 'OpenAI founded',
    detail: 'Sam Altman, Elon Musk, and other Silicon Valley figures launched OpenAI as a nonprofit AI research company with a $1 billion commitment. The founding charter explicitly stated the goal of ensuring artificial general intelligence benefits all of humanity. The organization was structured to prioritize safety over profits and promised to publish most research openly, countering concerns about AI development happening behind closed doors at large corporations.',
    personality: 'Sam Altman, Elon Musk',
    category: 'foundation',
    implications: 'OpenAI embedded alignment into the charter of a major AI laboratory, signaling that safety could be a core mission rather than an afterthought. Its initial open approach (later modified) aimed to democratize AI research. The organization became a primary driver of capability advances with GPT models while maintaining safety teams. OpenAI\'s trajectory—from nonprofit to capped-profit, from open to more closed—reflects ongoing tensions between safety, competitive pressures, and commercial opportunities.',
  },
  {
    year: 2016,
    event: 'AlphaGo defeats Lee Sedol',
    detail: 'DeepMind\'s AlphaGo system defeated Lee Sedol, one of the world\'s top Go players, 4-1 in a highly publicized match. Go was considered a grand challenge for AI because its vast search space (more possible positions than atoms in the universe) seemed to require human-like intuition. AlphaGo combined deep neural networks with Monte Carlo tree search, learning from human games then surpassing human capability through self-play. Move 37 of Game 2 was particularly stunning—a move so unconventional that commentators initially thought it was a mistake.',
    personality: 'DeepMind',
    category: 'capability',
    implications: 'AlphaGo demonstrated that AI could achieve superhuman performance in domains requiring creativity and intuition, not just brute-force calculation. The victory shocked both the AI research community and the broader public, advancing timelines for when human-level AI might emerge. The match crystallized concerns about the pace of AI progress—if Go could fall so quickly, what remained safe? Later versions (AlphaGo Zero, AlphaZero) learned entirely from self-play, suggesting less dependence on human knowledge.',
  },
  {
    year: 2016,
    event: '"AI Alignment: Why It\'s Hard and Where to Start"',
    detail: 'MIRI and Eliezer Yudkowsky published a comprehensive presentation explaining the core technical difficulties of AI alignment. The work decomposed the problem into subproblems: avoiding negative side effects, avoiding reward hacking, difficulty of specifying human values, and maintaining safety during self-improvement. It emphasized that alignment was not merely a matter of instructing AI to "be good" but required solving hard technical problems around goal specification and preservation.',
    personality: 'Eliezer Yudkowsky, MIRI',
    category: 'alignment',
    implications: 'This document helped establish the intellectual framework for modern alignment research by clearly articulating what problems needed to be solved. Its taxonomy of alignment failures—side effects, reward hacking, specification gaming—provided vocabulary for discussing problems that later emerged in deployed systems. The work influenced research agendas at newly-formed safety teams and helped new researchers understand why alignment was technically difficult rather than merely a matter of good intentions.',
  },
  {
    year: 2017,
    event: 'Transformer architecture introduced',
    detail: 'Researchers at Google Brain published "Attention Is All You Need," introducing the transformer architecture that would revolutionize AI. Unlike previous recurrent neural networks that processed sequences step-by-step, transformers used attention mechanisms to process entire sequences in parallel, enabling much larger models and faster training. The architecture made it practical to train models on unprecedented amounts of text data, leading directly to GPT, BERT, and subsequent large language models.',
    personality: 'Vaswani et al., Google Brain',
    category: 'capability',
    implications: 'The transformer became the foundation of the LLM revolution, enabling the scaling that produced GPT-3, GPT-4, and other frontier models. Its architectural elegance allowed straightforward scaling—add more parameters, more data, more compute—leading to the scaling laws that now drive AI development. The paper is the most cited AI paper in history and transformed the field by demonstrating that a single architecture could excel across vision, language, and multimodal tasks.',
  },
  {
    year: 2018,
    event: 'BERT and RLHF mature',
    detail: 'Google released BERT (Bidirectional Encoder Representations from Transformers), demonstrating the power of pre-training on large text corpora. Meanwhile, OpenAI advanced Reinforcement Learning from Human Feedback (RLHF), a technique for fine-tuning AI models based on human preferences. RLHF emerged as a practical method for aligning model behavior with human intentions by training on comparisons of outputs rather than explicit specifications of correct behavior.',
    personality: 'Google, OpenAI',
    category: 'alignment',
    implications: 'RLHF became the dominant paradigm for making large language models more helpful and less harmful. It represented a significant practical advance in alignment—finally, there was a technique that worked at scale. ChatGPT\'s success relied heavily on RLHF. However, RLHF has limitations: it can amplify biases in human feedback, be gamed by models, and may teach sycophancy rather than truthfulness. Understanding these limitations drives current alignment research on alternatives like constitutional AI.',
  },
  {
    year: 2020,
    event: 'GPT-3 released',
    detail: 'OpenAI released GPT-3, a 175-billion parameter language model that demonstrated emergent capabilities absent in smaller models. GPT-3 could perform tasks never explicitly trained for—translation, coding, arithmetic, even simple reasoning—simply by being prompted with examples. The model showed that scaling could produce qualitatively new abilities, not just quantitative improvements. Access was provided through an API, sparking a wave of startup activity building on AI capabilities.',
    personality: 'OpenAI',
    category: 'capability',
    implications: 'GPT-3 crystallized the scaling hypothesis—that pushing more compute into larger models yields increasingly general capabilities. This reoriented the AI industry toward ever-larger models and helped initiate the current AI investment boom. The model also demonstrated concerning capabilities: generating convincing misinformation, producing subtly biased content, and occasionally synthesizing harmful information. These dual-use concerns intensified discussions of responsible deployment and spurred safety research.',
  },
  {
    year: 2022,
    event: 'ChatGPT released',
    detail: 'OpenAI released ChatGPT, a conversational AI fine-tuned with RLHF to be helpful, harmless, and honest. The interface—a simple chat window—made advanced AI accessible to millions overnight. ChatGPT reached 100 million users within two months, the fastest-growing consumer application in history. Users found it could help with writing, coding, analysis, and creative tasks. It also occasionally hallucinated, made errors confidently, and could be jailbroken to produce harmful content.',
    personality: 'OpenAI',
    category: 'capability',
    implications: 'ChatGPT brought AI capabilities and alignment challenges into mainstream awareness simultaneously. Suddenly, everyone from students to CEOs was using AI daily, experiencing both its remarkable abilities and its failures. The launch triggered an AI arms race among tech companies and massive investment in AI startups. It also made alignment concrete for the public—jailbreaks and hallucinations demonstrated in real-time why making AI systems behave beneficially requires ongoing work.',
  },
  {
    year: 2023,
    event: 'First AI Safety Summit at Bletchley Park',
    detail: 'The UK hosted the first global AI Safety Summit at Bletchley Park, the historic site where Turing worked on breaking the Enigma code. Representatives from 28 nations, including the US, China, and EU members, along with major AI companies signed the Bletchley Declaration acknowledging the need to collaborate on AI safety. The summit focused on frontier AI risks and established agreement on testing and evaluation frameworks.',
    personality: 'UK Government, major AI labs',
    category: 'safety',
    implications: 'The summit marked AI safety\'s arrival as a major international policy priority, comparable to nuclear non-proliferation or climate negotiations. It was remarkable for including China alongside Western nations in discussions of AI governance. The commitments made, while not legally binding, established norms around safety testing and transparency. Follow-up summits in France and South Korea continued building international frameworks, suggesting AI governance may become a permanent feature of geopolitics.',
  },
  {
    year: 2024,
    event: 'AI Safety Institutes established',
    detail: 'The United States, United Kingdom, and several other nations established AI Safety Institutes to research and evaluate frontier AI systems. The US AISI operates under NIST, conducting technical evaluations of advanced models. The UK AISI partners directly with labs like OpenAI and Anthropic for pre-deployment testing. These institutes employ researchers from academia and industry to develop evaluation methodologies, red-teaming approaches, and safety standards.',
    personality: 'CAIS, Redwood Research, government institutes',
    category: 'safety',
    implications: 'Government-backed AI safety institutes institutionalized oversight of frontier AI development. They represent a shift from industry self-regulation toward structured external review. These institutes are developing the frameworks that may eventually become mandatory requirements before deploying advanced systems. Their work on evaluation science—how to test for dangerous capabilities like biological weapons design or autonomous hacking—is creating tools for responsible AI governance.',
  },
  {
    year: 2025,
    event: 'Comprehensive AI alignment surveys',
    detail: 'Major academic surveys consolidate the field of AI alignment, reviewing hundreds of papers and establishing a taxonomy of problems and approaches. Alignment is now recognized as a distinct technical discipline with its own conferences (AAAI AI Safety track), journals, and degree programs. Research covers technical approaches (RLHF, constitutional AI, interpretability, adversarial training) as well as governance and deployment practices.',
    personality: 'Academia and industry researchers',
    category: 'alignment',
    implications: 'The formalization of alignment as an academic field ensures systematic progress and knowledge accumulation. Graduate programs now train researchers specifically in alignment, creating a pipeline of talent. Standard benchmarks allow comparison of approaches. The field\'s maturation also enables more precise discussions—rather than vague appeals to "safe AI," researchers can point to specific techniques addressing specific problems. This technical grounding is essential for building AI systems we can actually trust.',
  },
];

// Category colors for timeline
export const CATEGORY_COLORS = {
  foundation: '#64748b',
  safety: '#22c55e',
  alignment: '#8b5cf6',
  singularity: '#f97316',
  capability: '#00f0ff',
};

// Category labels
export const CATEGORY_LABELS = {
  foundation: 'AI Foundation',
  safety: 'AI Safety',
  alignment: 'Alignment',
  singularity: 'Singularity',
  capability: 'Capability',
};
